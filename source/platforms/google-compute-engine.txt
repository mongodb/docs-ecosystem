.. _google-compute-engine

=====================
Google Compute Engine
=====================

Overview
--------

This guide is intended to provide guidance and best practices for getting started with MongoDB on `Google Compute Engine (GCE) <https://cloud.google.com/products/compute-engine/>`_. The guide covers recommendations for choosing appropriate instance and storage configurations, then walks through deploying MongoDB on a single instance, concluding with backup/restore and other deployment considerations.

Instance Recommendations
------------------------

GCE offers several different instance types across four categories:

- Standard
- Shared Core
- High Memory
- High CPU

When deploying MongoDB, consider the following recommendations based on deployment type and component.

- For development deployments, deploy :program:`mongod` on Standard or High Memory instances.
- For production deployments, deploy :program:`mongod` on High Memory instances.
- For sharded clusters :program:`mongos` should be run on instances on which your application is running however, it can be run as a standalone process on Standard n1-standard-1 instances.
- Sharded cluster config servers should be deployed on Standard n1-standard-1 instances.

.. note::

    Shared Core instances are not recommended as they only provide intermittent access the underlying CPU. High CPU instances are also not recommended because MongoDB is traditionally memory and I/O bound.

Storage Considerations
----------------------

GCE offers persistent disk storage to go along with your instances, either as a single root volume or with multiple disks attached to an instance. There are multiple storage configurations possible with GCE persistent disks, each with it's own tradeoffs. To properly determine the right storage configuration you will need to prototype and test your workload.

Configurations
~~~~~~~~~~~~~~

- Single root volume
- Individual volumes, one for each of the following: data, log, and journal.
- Multiple volumes for data, striped with `LVM <http://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)>`_
- Multiple volumes for data, configured with RAID0, RAID1, or RAID10 using `mdadm <http://en.wikipedia.org/wiki/Mdadm>`_

Performance
~~~~~~~~~~~

GCE persistent disk performance profiles are primarily based on allocated disk size. At higher disk sizes performance can also scale based on the number of cores within an instance. For more information, refer to "Persistent Disk Performance" within the `GCE Disks documentation <https://developers.google.com/compute/docs/disks>`_.

Instance Configuration
----------------------

The following steps can be used to deploy MongoDB on a single GCE instance. The instance will be configured as follows:

- CentOS 6
- MongoDB 2.4.x installed via Yum
- Individual persistent disks for data, journal, and log
- Updated read-ahead values for each disk
- Updated ulimit settings
- Updated TCP keep-alive settings

.. note::

    To get started, please have the Google Cloud SDK tools installed and setup to work with your account, as well as an initial project created. For more information, refer to the `Google Cloud SDK installation instructions <https://developers.google.com/cloud/sdk/#Getting_Started>`_, `Managing Authentication and Credentials <https://developers.google.com/cloud/sdk/gcloud#gcloud.auth>`_, and `Quick Start <https://developers.google.com/compute/docs/signup>`_.

First create the persistent disk that will be the root storage device for the instance using the ``gcutil`` command.

.. code-block:: sh

    $ gcutil adddisk mongodb-root --project=mongodb-project --size_gb=10 --zone=us-central1-b

Next create the persistent disk that will hold MongoDB data.

.. code-block:: sh

    $ gcutil adddisk mongodb-data --project=mongodb-project --size_gb=500 --zone=us-central1-b

Now repeat that process to create additional persistent disks for the journal and log.

.. code-block:: sh

    $ gcutil adddisk mongodb-journal --project=mongodb-project --size_gb=25 --zone=us-central1-b
    $ gcutil adddisk mongodb-log --project=mongodb-project --size_gb=25 --zone=us-central1-b

With the disks now created, the next step is to launch an instance with these volumes attached.

.. code-block:: sh

    $ gcutil addinstance mongodb-instance --project=mongodb-project --zone=us-central1-b \
    --machine_type=n1-standard-2 --image=projects/centos-cloud/global/images/centos-6-v20131120 \
    --disk=mongodb-root,mode=rw,boot --disk=mongodb-data,mode=rw \
    --disk=mongodb-journal,mode=rw, --disk=mongodb-log,mode=rw

The above command will launch an `n1-standard-2` instance running CentOS in the `us-central1-b` zone with 4 disks attached (1 as instance boot volume and the others for MongoDB data, log, and journal).

Once the instance is created (check status using the ``gcutil getinstance`` command) connect to it via SSH.

.. code-block:: sh

    $ gcutil ssh mongodb-instance

Use :program:`yum` to update the system and then add in the MongoDB yum repo.

.. code-block:: sh

    $ sudo yum -y update

    $ echo "[MongoDB]
    name=MongoDB Repository
    baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86_64
    gpgcheck=0
    enabled=1" | sudo tee -a /etc/yum.repos.d/mongodb.repo

Then install MongoDB:

    $ sudo yum install -y mongo-10gen-server

Now locate the persistent disks previously attached to the instance so they can be used for storage.

.. code-block:: sh

    $ ls -l /dev/disk/by-id/google-*
    ...
    lrwxrwxrwx. 1 root root  9 Mar  3 20:38 google-mongodb-data -> ../../sdb
    lrwxrwxrwx. 1 root root  9 Mar  3 20:38 google-mongodb-journal -> ../../sdc
    lrwxrwxrwx. 1 root root  9 Mar  3 20:38 google-mongodb-log -> ../../sdd
    lrwxrwxrwx. 1 root root  9 Mar  3 20:38 google-mongodb-root -> ../../sda

Next, format each disk (skip the root volume as it was formatted and attached at instance boot):

.. code-block:: sh

    $ sudo mkfs.ext4 /dev/sdb
    $ sudo mkfs.ext4 /dev/sdc
    $ sudo mkfs.ext4 /dev/sdd

Now create mount points, mount, and set the appropriate ownership ("owner:group" should be ``mongod``:``mongod``).

.. code-block:: sh

    $ sudo mkdir /data /journal /log

    $ echo '/dev/sdb /data ext4 defaults,auto,noatime,noexec 0 0
    /dev/sdc /journal ext4 defaults,auto,noatime,noexec 0 0
    /dev/sdd /log ext4 defaults,auto,noatime,noexec 0 0' | sudo tee -a /etc/fstab

    $ sudo mount /data
    $ sudo mount /journal
    $ sudo mount /log

    $ sudo chown mongod:mongod /data /journal /log

.. note:

    To relocate the journal to the newly attached mount point, issue the following command

    .. code-block:: sh

        $ sudo ln -s /journal /data/journal

Storage setup is now complete. Now configure the following MongoDB parameters by editing the configuration file :file:`/etc/mongod.conf`:

.. code-block:: sh

    dbpath = /data
    logpath = /log/mongod.log

Optionally, if you don't want MongoDB to start at boot you can issue the following command:

.. code-block:: sh

    $ sudo chkconfig mongod off

By default CentOS uses :program:`ulimit` settings that are not appropriate for MongoDB. To setup :program:`ulimit` to match the documented :manual:`ulimit settings </reference/ulimit>` use the following steps:

.. code-block:: sh

    $ sudo nano /etc/security/limits.conf
    mongod soft nofile 64000
    mongod hard nofile 64000
    mongod soft nproc 32000
    mongod hard nproc 32000

    $ sudo nano /etc/security/limits.d/90-nproc.conf
    mongod soft nproc 32000
    mongod hard nproc 32000

Additionally, default read ahead settings on persistent disks are not optimized for MongoDB. As noted in the read-ahead settings from :manual:`Production Notes </administration/production-notes>`, the settings should be adjusted to read approximately 32 blocks (or 16 KB) of data. The following command will set the readahead appropriately (repeat for necessary volumes):

.. code-block:: sh

    $ sudo blockdev --setra 32 /dev/sdb

To make this change persistent across system boot you can issue the following command. Repeat this command for all required volumes (:file:`/dev/sdc` and :file:`/dev/sdd`).

.. code-block:: sh

    $ echo 'ACTION=="add", KERNEL=="sdb", ATTR{bdi/read_ahead_kb}="16"' | sudo tee -a /etc/udev/rules.d/85-mongod.rules

The default TCP keep alive on Linux also needs to be changed for MongoDB deployments. The following commands will update the time to 300 seconds and persist those settings across reboot:

.. code-block:: sh

    $ echo 300 | sudo tee /proc/sys/net/ipv4/tcp_keepalive_time
    $ echo "net.ipv4.tcp_keepalive_time = 300" | sudo tee -a /etc/sysctl.conf

To startup MongoDB, issue the following command:

.. code-block:: sh

    $ sudo service mongod start

And now connect to the MongoDB instance using the :program:`mongo` shell:

.. code-block:: sh

    $ mongo
    MongoDB shell version: 2.4.8
    connecting to: test
    >

To have MongoDB startup automatically at boot issue the following command:

.. code-block:: sh

    $ sudo chkconfig mongod on

Backup and Restore
------------------

GCE offers the ability to take snapshots of your persistent disk and create new persistent disks from that snapshot. The underlying mechanism uses differential snapshots, which allow for better performance and lower storage costs. Snapshots are also a `global resouce <https://developers.google.com/compute/docs/overview#globalresources>`_ meaning they are available in any region in which your project is setup.

To backup your MongoDB data using GCE snapshots, use the following procedure. First use :program:`db.fsyncLock` from the :program:`mongo` shell to force :program:`mongod` to flush all pending writes to disk and lock it against further writes.

.. code-block:: sh

    > db.fsyncLock()
    {
        "info" : "now locked against writes, use db.fsyncUnlock() to unlock",
        "seeAlso" : "http://dochub.mongodb.org/core/fsynccommand",
        "ok" : 1
    }

Next force the system to sync the filesystem and flush disk buffers:

.. code-block:: sh

    $ sudo sync

Now execute the snapshot command:

.. code-block:: sh

    $ gceutil addsnapshot mongodb-data-snapshot --project=mongodb-project --source_disk=mongodb-data

Once the snapshot has been completed, unlock :program:`mongod` via the :program:`mongo` shell:

.. code-block:: sh

    > db.fsyncUnlock()

To view snapshot details, use the following command:

.. code-block:: sh

    $ gcutil getsnapshot mongodb-data-snapshot

    +----------------------+------------------------------------+
    | name                 | mongodb-data-snapshot              |
    | description          |                                    |
    | creation-time        | 2014-03-03T15:13:50.209-08:00      |
    | status               | READY                              |
    | disk-size-gb         | 500                                |
    | storage-bytes        |                                    |
    | storage-bytes-status |                                    |
    | source-disk          | us-central1-b/disks/mongodb-data   |
    +----------------------+------------------------------------+

To restore MongoDB data from a GCE snapshot, use the following procedure. First, if :program:`mongod` is running, stop it.

.. code-block:: sh

    $ sudo service mongod stop

Next create a new persistent disk using the snapshot as the source and attach it to a running instance.

.. code-block:: sh

    $ gcutil adddisk mongodb-data-2 --source_snapshot=mongodb-data-snapshot --project=mongodb-project --zone=us-central1-b
    $ gcutil attachdisk mongodb-instance --disk=mongodb-data-2,mode=rw --project=mongodb-project

Once the disk is attached refer to the above instructions for mounting the disk to the :file:`/data` mount point, then restart :program:`mongod`:

.. code-block:: sh

    $ sudo service mongod start

Deployment Notes
----------------

Networking
~~~~~~~~~~

Networks in GCE are a `global resouce <https://developers.google.com/compute/docs/overview#globalresources>`_ meaning they are available in any region in which your project is setup. By default, all new instances have the following connections enabled:

- Traffic between instances in the same network, over any port and any protocol
- Incoming SSH (port 22) access from anywhere

Unless otherwise specified during ``gcutil addinstance`` new instances are assigned to the ``default`` network that exists within your account and instances can only belong to a single network. For more information on creating and managing network, refer to the `Networking and Firewalls <https://developers.google.com/compute/docs/networking>`_ documentation.

Regions and Zones
~~~~~~~~~~~~~~~~~

GCE provides different regions and zones to provide control over where data is stored and used. Certain resources exist only at zone, region, or global levels. For example, instances and disks are zone-specific however snapshots and networks are globally accessible. For more information, refer to the `Regions and Zones <https://developers.google.com/compute/docs/zones>`_ documentation.

When deploying MongoDB for high availability within GCE, we stronly recommend deploying across multiple zones or multiple regions. Introducing multiple regions into your deployment may increase transfer costs and latency of network access between individual nodes so it is best to prototype this approach prior to a production deployment.

For more information on high availability in GCE, refer to `Designing Robust Systems <https://developers.google.com/compute/docs/robustsystems>`_.

