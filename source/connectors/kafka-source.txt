.. _kafka-source:

============================
Kafka Source Connector Guide
============================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

The MongoDB Kafka Source Connector moves data from a MongoDB replica set
into a Kafka cluster. The connector configures and consumes :manual:`change
stream event documents </changeStreams>` and publishes them to a topic.

Change streams, a feature introduced in MongoDB 3.6, generate event
documents that contain changes to data stored in MongoDB in real-time and
provide guarantees of durability, security, and idempotency. You can configure
change streams to observe changes at the **collection**, **database**, or
**deployment** level. See `An Introduction to Change Streams
<https://www.mongodb.com/blog/post/an-introduction-to-change-streams>`_
for more information.

.. note::

   Change streams require a replicaSet or a sharded cluster using replicaSets.

Change Stream Event Document Format
-----------------------------------

A change stream event document contains several fields that describe the
change:

* The top-level ``_id`` field is used as the :manual:`resume token
  </changeStreams/#change-stream-resume>` which is
  used to start a change stream from a specific point in time.

* The ``operationType`` field identifies the type of change represented in
  the change stream document. Possible values include: "insert", "update",
  "replace", "delete", "invalidate", "drop", "dropDatabase", and "rename".

* The ``fullDocument`` field contents depend on the operation as follows:

  - For insert and replace operations, it contains the new document being
    inserted or replacing the existing document.
  - For update operations, it contains the complete document that is being
    updated at some point in time after the update occurred. If the
    document was deleted since the update, it contains a null value.

* The ``documentKey`` contains either the ``_id`` field of the document
  that was updated or all the components of a shard key for sharded
  collections.

* The ``txnNumber`` and ``lsid`` identify the transaction if the change
  occurred within one.

.. code-block:: json

   {
     _id: { <BSON Object> },
     "operationType": "<operation>",
     "fullDocument": { <document> },
     "ns": {
       "db": <database>,
       "coll": <collection>
     },
     "to": {
       "db": <database>,
       "coll": <collection>
     },
     "documentKey": {
       _id: <value>
     },
     "updateDescription": {
       "updatedFields": { <document> },
       "removedFields": [ <field>, ... ]
     },
     "clusterTime": <Timestamp>,
     "txnNumber": <NumberLong>,
     "lsid": {
       "id": <UUID>,
       "uid": <BinData>
     }
   }

Source Connector Configuration Properties
-----------------------------------------

The MongoDB Kafka Source Connector uses the following settings to create
change streams and customize the output to save to the Kafka cluster. For
an example source connector configuration file, see
`MongoSourceConnector.properties
<https://github.com/mongodb/mongo-kafka/blob/master/config/MongoSourceConnector.properties>`_.

.. list-table::
   :header-rows: 1
   :stub-columns: 1

   * - Name
     - Type
     - Description

   * - connection.uri
     - string
     - | A :manual:`MongoDB connection URI string </reference/connection-string/#standard-connection-string-format>`.
       | **Default**: ``mongodb://localhost:27017,localhost:27018,localhost:27019``
       | **Accepted Values**: A valid MongoDB connection URI string

   * - database
     - string
     - | Name of the database to watch for changes. If not set, all databases are watched.
       | **Default**: ""
       | **Accepted Values**: A single database name

   * - collection
     - string
     - | Name of the collection in the database to watch for changes.
       | The collection in the database to watch. If not set then all collections will be watched.
       | **Default**: ""
       | **Accepted Values**: A single collection name

   * - publish.full.document.only
     - boolean
     - | Only publish the changed document instead of the full change stream document. Sets the ``change.stream.full.document=updateLookup`` automatically so updated documents will be included.
       | **Default**: false
       | **Accepted Values**: ``true`` or ``false``

   * - pipeline
     - string
     - | An array of objects describing the pipeline operations to run.

       .. example::

          .. code-block:: shell

             [{"$match": {"operationType": "insert"}}, {"$addFields": {"Kafka": "Rules!"}}]

       .. seealso:: :ref:`Custom pipeline example <custom-pipeline-example>`.

       | **Default**: []
       | **Accepted Values**: A valid JSON array

   * - collation
     - string
     - | A JSON :manual:`collation document </reference/collation/#collation-document>` that contains options to use for the change stream. Append ``.asDocument().toJson()`` to the collation document to create the JSON representation.
       | **Default**: ""
       | **Accepted Values**: A valid JSON document representing a collection

   * - batch.size
     - int
     - | The cursor batch size.
       | **Default**: 0
       | **Accepted Values**: An integer

   * - change.stream.full.document
     - string
     - | Determines what to return for update operations when using a Change Stream. When set to 'updateLookup', the change stream for partial updates will include both a delta describing the changes to the document as well as a copy of the entire document that was changed from *some point in time* after the change occurred.
       | **Default**: ""
       | **Accepted Values**: "" or ``default`` or ``updateLookup``

   * - poll.await.time.ms
     - long
     - | The amount of time to wait before checking for new results on the change stream
       | **Default**: 5000
       | **Accepted Values**: An integer

   * - poll.max.batch.size
     - int
     - | Maximum number of change stream documents to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.
       | **Default**: 1000
       | **Accepted Values**: An integer

   * - topic.prefix
     - string
     - | Prefix to prepend to database & collection names to generate the name of the Kafka topic to publish data to.

       .. seealso:: :ref:`Topic naming example <topic-naming-example>`.

       | **Default**: ""
       | **Accepted Values**: A string

.. note::

   The default maximum size for Kafka messages is 1MB. Update the
   following Kafka (versions 0.11.0 through 2.2) configuration properties to
   enable a larger maximum size if the JSON string size of the change stream
   documents exceeds the maximum:

   .. list-table::
      :header-rows: 1
      :stub-columns: 2

      * - System
        - Property Name
        - Description

      * - Consumer
        - `max.partition.fetch.bytes
          <https://kafka.apache.org/22/documentation.html#consumerconfigs>`_
        - Maximum size of a message that can be fetched by a consumer.

      * - Broker
        - `replica.fetch.max.bytes
          <https://kafka.apache.org/22/documentation.html#brokerconfigs>`_
        - Maximum size of a message that can be replicated within a Kafka
          cluster.

      * - Broker
        - `message.max.bytes
          <https://kafka.apache.org/22/documentation.html#brokerconfigs>`_
        - Maximum size of a message from a producer that is accepted by the
          broker.

      * - Producer
        - `max.message.bytes
          <https://kafka.apache.org/22/documentation.html#producerconfigs>`_
        - Per referenced topic, the maximum size of an uncompressed message that can be appended to a
          topic.

.. _custom-pipeline-example:

Custom Pipeline Example
-----------------------

You can use the ``pipeline`` configuration setting to define a custom
:manual:`aggregation pipeline <reference/operator/aggregation-pipeline/>`
to filter or modify the change events output. In this example, we set the
``pipeline`` configuration to observe only insert change events:

.. code-block:: properties

   pipeline=[{"$match": {"operationType": "insert"}}]

.. note::

   Make sure the results of the aggregation pipeline contain the top-level
   ``_id`` field which MongoDB uses as the :manual:`resume token
   </changeStreams/#change-stream-resume-token/>`.

.. _topic-naming-example:

Topic Naming Example
--------------------

The MongoDB Kafka Source connector publishes the changed data events to
a Kafka topic that consists of the database and collection name from which
the change originated. For example, if an insert was performed on the
``test`` database and ``data`` collection, the connector will publish the
data to a topic named ``test.data``.

If the ``topic.prefix`` configuration is set to **true**, the Kafka topic
name will be prepended with the specified value. For example:

.. code-block:: properties

   topic.prefix=mongo

Once set, any data changes to the ``data`` collection in the ``test`` database
are published to a topic named ``mongo.test.data``.
