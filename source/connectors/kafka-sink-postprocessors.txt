

====================================
Kafka Sink Connector Post-Processors
====================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecols

Post Processing of Documents
----------------------------

Post processors are sink connector classes that modify data in the
``SinkDocument``, a class that contains a BSON representation of the
``SinkRecord`` key and value fields, after it has been read from the
Kafka topic. The connector applies a **chain of post processors** in
which each post processor is executed in the order provided on
the ``SinkDocument``, and the result is stored in a MongoDB collection.

Post processors perform data modification tasks such as setting
the document ``_id`` field, key or value field projection, renaming
fields, and redacting sensitive information. You can use the
following pre-built post processors or implement your own by extending
the `PostProcessor
<https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/PostProcessor.java>`_
class:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1,2

   * - Post Processor Name
     - Description

   * - DocumentIdAdder
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.DocumentIdAdder``
       | Uses a configured *strategy* to insert an ``_id`` field.

       .. seealso:: :ref:`Strategy options and configuration <config-document-id-adder>`.

   * - BlacklistKeyProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.BlacklistKeyProjector``
       | Removes matching key fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <blacklist-example>`.

   * - BlacklistValueProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.BlacklistValueProjector``
       | Removes matching value fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <blacklist-example>`.

   * - WhitelistKeyProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.WhitelistKeyProjector``
       | Includes only matching key fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <whitelist-example>`.

   * - WhitelistValueProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.WhitelistValueProjector``
       | matching value fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <whitelist-example>`.

   * - KafkaMetaAdder
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder``
       | Adds a field composed of the concatenation of Kafka topic, partition, and offset to the document.

   * - RenameByMapping
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.field.renaming.RenameByMapping``
       | Renames fields that are an exact match to a specified key or value field.

       .. seealso:: :ref:`Renamer configuration <config-field-renamer>` and :ref:`Example <field-renamer-mapping-example>`.

   * - RenameByRegex
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.field.renaming.RenameByRegex``
       | Renames fields that match a regular expresion.

       .. seealso:: :ref:`Renamer configuration <config-field-renamer>` and :ref:`Example <field-renamer-regex-example>`.


You can configure the post processor chain by specifying an ordered,
comma separated list of fully-qualified ``PostProcessor`` class names:

.. code-block:: properties

   post.processor.chain=com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder,com.mongodb.kafka.connect.sink.processor.WhitelistValueProjector

.. note::

   The ``DocumentIdAdder`` post processor is automatically added to the
   first position in the chain if not already present.

Configuration Options
---------------------

This section explains the available configuration options for post
processors included in the MongoDB Kafka Connector.

.. _config-document-id-adder:

DocumentIdAdder
~~~~~~~~~~~~~~~

The ``DocumentIdAdder`` post processor provides the ``_id`` field for
a ``SinkDocument`` before it is written to a MongoDB collection. This
post processor is configured using a **strategy** that contains the logic
for generating the value of the ``_id``. The following strategies are
provided with this connector:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1,2

   * - Strategy Name
     - Description

   * - BsonOidStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy``
       | Default value for the ``DocumentIdAdder`` post processor.
       | Generates a MongoDB **BSON ObjectId**.

   * - KafkaMetaDataStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.KafkaMetaDataStrategy``
       | Builds a string composed of the concatenation of Kafka topic, partition, and offset.

   * - FullKeyStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.FullKeyStrategy``
       | Uses complete key structure of the ``SinkDocument``.
       | Defaults to a blank document if no key exists.

   * - ProvidedInKeyStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy``
       | Uses the ``_id`` field specified in the key structure of the ``SinkDocument`` if it exists.
       | Throws an exception if the field is missing.

   * - ProvidedInValueStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy``
       | Uses the ``_id`` field specified in the value structure of the ``SinkDocument`` if it exists.
       | Throws an exception if the field is missing.

   * - PartialKeyStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.PartialKeyStrategy``
       | Uses a blacklist or whitelist projection of the key structure of the ``SinkDocument``.
       | Defaults to a blank document if no key exists.

   * - PartialValueStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy``
       | Uses a blacklist or whitelist projection of the value structure of the ``SinkDocument``.
       | Defaults to a blank document if no value exists.

   * - UuidStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy``
       | Generates a random UUID as a string.

You can assign the ``document.id.strategy`` property as follows:

.. code-block:: properties

   document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy

To define a custom strategy, create a class that implements the interface
`IdStrategy
<https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/id/strategy/IdStrategy.java>`_
and provide the fully-qualified path to the ``document.id.strategy``
setting.

.. admonition:: Selected strategy may have implications on delivery semantics
   :class: note

   BSON ObjectId or UUID strategies can only guarantee at-least-once
   delivery since new ids would be generated on retries or re-processing.
   Other strategies permit exactly-once delivery if the fields that form
   the document *_id* are guaranteed to be unique.

.. _config-blacklist-whitelist:

Blacklist / Whitelist Projector
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section provides example projection configurations to show how they
filter the following sample record:

.. code-block:: json

   {
     "name": "Anonymous",
     "age": 42,
     "active": true,
     "address": {
       "city": "Unknown",
       "country": "NoWhereLand"
     },
     "food": [
       "Austrian",
       "Italian"
     ],
     "data": [
       {
         "k": "foo",
         "v": 1
       }
     ],
     "lut": {
       "key1": 12.34,
       "key2": 23.45
     }
   }

.. _blacklist-example:

Blacklist Projection Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the following example sink configuration, we specify a blacklist
projection and the specific fields to omit from the record:

.. code-block:: properties

   post.processor.chain=com.mongodb.kafka.connect.sink.processor.Blacklist[Key|Value]Projector
   [key|value].projection.type=blacklist
   [key|value].projection.list=age,address.city,lut.key2,data.v

.. note::

   You can use the "." notation to reference subdocuments in the record. You
   can also use it to reference fields of documents within an array.

The record contains the following data after applying the projection:

.. code-block:: json

   {
     "name": "Anonymous",
     "active": true,
     "address": {
       "country": "NoWhereLand"
     },
     "food": [
       "Austrian",
       "Italian"
     ],
     "data": [
       {
         "k": "foo"
       }
     ],
     "lut": {
       "key1": 12.34
     }
   }

.. _whitelist-example:

Whitelist Projection Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the following example sink configuration, we specify a whitelist
projection and the specific fields to include in the record:

.. code-block:: properties

   post.processor.chain=com.mongodb.kafka.connect.sink.processor.Whitelist[Key|Value]Projector
   [key|value].projection.type=whitelist
   [key|value].projection.list=age,address.city,lut.key2,data.v

.. note::

   You can use the "." notation to reference subdocuments in the record. You
   can also use it to reference fields of documents within an array.

The record contains the following data after applying the projection:

.. code-block:: json

   {
     "age": 42,
     "address": {
       "city": "Unknown"
     },
     "data": [
       {
         "v": 1
       }
     ],
     "lut": {
       "key2": 23.45
     }
   }

Wildcard Pattern Matching
~~~~~~~~~~~~~~~~~~~~~~~~~

The configuration supports wildcard matching using a **'*'** character notation. A wildcard
is supported on any level in the document structure in order to include (whitelist) or
exclude (blacklist) any fieldname at the corresponding level. A part from that there is support
for **'**'** which can be used at any level to include/exclude the full sub structure
(i.e. all nested levels further down in the hierarchy).


.. note::

   A bunch of more concrete examples of field projections including wildcard
   pattern matching can be found in a corresponding `test class
   <https://github.com/mongodb/mongo-kafka/blob/master/src/test/java/com/mongodb/kafka/connect/sink/processor/field/projection/FieldProjectorTest.java>`_.

Wildcard Whitelist Example
^^^^^^^^^^^^^^^^^^^^^^^^^^

The following example will include the *age* field, the *lut* field and all its immediate sub-fields (i.e. one level down):

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=age,lut.*

The following example will include the *active* field, the *address* field and its full sub structure (all available nested levels):

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=active,address.**

The final whitelist example will include all fields on the 1st and 2nd level:

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=*.*

Wildcard Blacklist Example
^^^^^^^^^^^^^^^^^^^^^^^^^^

The following example will exclude the *age* field, the *lut* field and all its immediate subfields (i.e. one level down):

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=age,lut.*

The following example will exclude the *active* field, the *address* field and its full sub structure (all available nested levels):

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=active,address.**

The final blacklist example will exclude: all fields on the 1st and 2nd level:

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=*.*

.. _config-field-renamer:

Field Renamer
~~~~~~~~~~~~~

There are two different options to rename any fields in the record, namely a simple and rigid 1:1 field name mapping or a more
flexible approach using regular expressions. Both config options are defined by inline JSON arrays containing objects which describe the renaming.


.. _field-renamer-mapping-example:

Example 1
^^^^^^^^^

.. code-block:: properties

   field.renamer.mapping=[{"oldName":"key.fieldA","newName":"field1"},{"oldName":"value.xyz","newName":"abc"}]

Will:


#. Rename field ``fieldA`` to ``field1`` in the **key document structure**
#. Rename field ``xyz`` to ``abc`` in the **value document structure**

.. _field-renamer-regex-example:

Example 2
^^^^^^^^^

.. code-block:: properties

   field.renamer.mapping=[{"regexp":"^key\\..*my.*$","pattern":"my","replace":""},{"regexp":"^value\\..*-.+$","pattern":"-","replace":"_"}]

These settings cause:

#. **All field names of the key structure containing 'my'** to be renamed so that **'my' is removed**
#. **All field names of the value structure containing a '-'** to be renamed by replacing **'-' with '_'**

.. note::

   The use of the **"." character** as navigational operator in both examples.
   It's used in order to refer to nested fields in subdocuments of the record
   structure. The prefix at the very beginning is used as a simple convention
   to distinguish between the *key* and *value* structure of a document.

.. _custom-write-models:

Custom Write Models
-------------------

The default behaviour for the connector whenever documents are written to MongoDB collections is to make use of a proper
`ReplaceOneModel <http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/ReplaceOneModel.html>`_ with
`upsert mode
<http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/ReplaceOneModel.html>`_
and **create the filter document based on the _id field** which results
from applying the configured ``DocumentIdAdder`` in the value structure of the sink document.

However, there are other use cases which need different approaches and the **customization option for generating custom write models**
can support these. The configuration entry (\ *mongodb.writemodel.strategy*\ ) allows for such customizations. Currently, the following
strategies are implemented:


* **default behaviour** com.mongodb.kafka.connect.sink.writemodel.strategy.\ **ReplaceOneDefaultStrategy**
* **business key** (see `use case 1 <https://github.com/mongodb/mongo-kafka#use-case-1-employing-business-keys>`_ below)
  com.mongodb.kafka.connect.sink.writemodel.strategy.\ **ReplaceOneBusinessKeyStrategy**
* **delete on null values** com.mongodb.kafka.connect.sink.writemodel.strategy.\ **DeleteOneDefaultStrategy** implicitly used when
  config option *mongodb.delete.on.null.values=true* for `convention-based deletion <https://github.com/mongodb/mongo-kafka#convention-based-deletion-on-null-values>`_
* **add inserted/modified timestamps** (see `use case 2 <https://github.com/mongodb/mongo-kafka#use-case-2-add-inserted-and-modified-timestamps>`_ below)
  com.mongodb.kafka.connect.sink.writemodel.strategy.\ **UpdateOneTimestampsStrategy**

.. note::

   Future versions will allow to make use of arbitrary, individual strategies
   that can be registered and easily used as *mongodb.writemodel.strategy*
   configuration setting.

Use Case 1: Employing Business Keys
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's say you want to re-use a unique business key found in your sink
records while at the same time have *BSON ObjectIds* created for
the resulting MongoDB documents.

To achieve this a few simple configuration steps are necessary:


#. Make sure to **create a unique key constraint** for the business key of your target MongoDB collection.
#. Use the ``PartialValueStrategy`` as the ``DocumentIdAdder``'s strategy in order to let the connector know which fields belong to the
   business key.
#. Use the ``ReplaceOneBusinessKeyStrategy`` instead of the default behaviour.

These configuration settings then allow to have **filter documents based
on the original business key but still have *BSON ObjectIds*
created for the _id field** during the first upsert into your target MongoDB target collection. Find below how such a setup might look like:

Given the following Kafka record:

.. code-block:: json

   {
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

Together with the sink connector config:

.. code-block:: json

   {
     "name": "mongo-sink",
     "config": {
       ...
       "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy",
       "value.projection.list": "fieldA,fieldB",
       "value.projection.type": "whitelist",
       "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy"
     }
   }

This will create a MongoDB document looking like:

.. code-block:: json

   {
     "_id": ObjectId('5abf52cc97e51aae0679d237'),
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

All upsert operations are done based on the unique business key which for this example is a compound one that consists of the two fields *(fieldA,fieldB)*.

Use Case 2: Add Inserted and Modified Timestamps
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's say you want to attach timestamps to the resulting MongoDB documents such that you can store the point in time of the document insertion and at the same time maintain a second timestamp reflecting when a document was modified.

All that needs to be done is use the **UpdateOneTimestampsStrategy** instead of the default behaviour. What results from this is that
the custom write model will take care of attaching two timestamps to MongoDB documents:

1) **_insertedTS**\ : will only be set once in case the upsert operation results in a new MongoDB document being inserted into the corresponding collection
2) **_modifiedTS**\ : will be set each time the upsert operation results in an existing MongoDB document being updated in the corresponding collection

Given the following Kafka record

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

Together with the sink connector config:

.. code-block:: json

   {
     "name": "mdb-sink",
     "config": {
       ...
       "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy",
       "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy"
     }
   }

This will create a MongoDB document looking like:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "_insertedTS": ISODate('2018-07-22T09:19:000Z"),
     "_modifiedTS": ISODate("2018-07-22T09:19:000Z"),
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

If at some point in time later there was a Kafka record referring to the same _id but containing updated data:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "fieldA": "anonymous",
     "fieldB": -23,
     "active": false,
     "values": [
       12.34,
       23.45
     ]
   }

Then the existing MongoDB document will get updated together with a fresh timestamp for the **_modifiedTS** value:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "_insertedTS": "ISODate('2018-07-22T09:19:000Z')",
     "_modifiedTS": "ISODate('2018-07-31T19:09:000Z')",
     "fieldA": "anonymous",
     "fieldB": -23,
     "active": false,
     "values": [
       12.34,
       23.45
     ]
   }
