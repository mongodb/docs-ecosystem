.kafka-sink-postprocessors

====================================
Kafka Sink Connector Post-Processors
====================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecols

Post Processing of Documents
----------------------------

Post processors are sink connector classes that modify data in the
``SinkDocument``, a class that contains a BSON representation of the
``SinkRecord`` key and value fields, after it has been read from the
Kafka topic. The connector applies a **chain of post processors** in
which each post processor is executed in the order provided on
the ``SinkDocument``, and the result is stored in a MongoDB collection.

Post processors perform data modification tasks such as setting
the document ``_id`` field, key or value field projection, renaming
fields, and redacting sensitive information. You can use the
following pre-built post processors or implement your own by extending
the `PostProcessor
<https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/PostProcessor.java>`_
class:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1,2

   * - Post Processor Name
     - Description

   * - DocumentIdAdder
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.DocumentIdAdder``
       | Uses a configured *strategy* to insert an ``_id`` field.

       .. seealso:: :ref:`Strategy options and configuration <config-document-id-adder>`.

   * - BlacklistKeyProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.BlacklistKeyProjector``
       | Removes matching key fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <blacklist-example>`.

   * - BlacklistValueProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.BlacklistValueProjector``
       | Removes matching value fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <blacklist-example>`.

   * - WhitelistKeyProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.WhitelistKeyProjector``
       | Includes only matching key fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <whitelist-example>`.

   * - WhitelistValueProjector
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.WhitelistValueProjector``
       | matching value fields from the sink record.

       .. seealso:: :ref:`Configuration <config-blacklist-whitelist>` and :ref:`Example <whitelist-example>`.

   * - KafkaMetaAdder
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder``
       | Adds a field composed of the concatenation of Kafka topic, partition, and offset to the document.

   * - RenameByMapping
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.field.renaming.RenameByMapping``
       | Renames fields that are an exact match to a specified key or value field.

       .. seealso:: :ref:`Renaming configuration <config-field-renaming>` and :ref:`Example <field-renaming-mapping-example>`.

   * - RenameByRegex
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.field.renaming.RenameByRegex``
       | Renames fields that match a regular expresion.

       .. seealso:: :ref:`Renaming configuration <config-field-renaming>` and :ref:`Example <field-renaming-regex-example>`.

You can configure the post processor chain by specifying an ordered,
comma separated list of fully-qualified ``PostProcessor`` class names:

.. code-block:: properties

   post.processor.chain=com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder,com.mongodb.kafka.connect.sink.processor.WhitelistValueProjector

.. note::

   The ``DocumentIdAdder`` post processor is automatically added to the
   first position in the chain if not already present.

Configuration Options
---------------------

This section explains the available configuration options for post
processors included in the MongoDB Kafka Connector.

.. _config-document-id-adder:

DocumentIdAdder
~~~~~~~~~~~~~~~

The ``DocumentIdAdder`` post processor provides the ``_id`` field for
a ``SinkDocument`` before it is written to a MongoDB collection. This
post processor is configured using a **strategy** that contains the logic
for generating the value of the ``_id``. The following strategies are
provided with this connector:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1,2

   * - Strategy Name
     - Description

   * - BsonOidStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy``
       | Default value for the ``DocumentIdAdder`` post processor.
       | Generates a MongoDB **BSON ObjectId**.

   * - KafkaMetaDataStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.KafkaMetaDataStrategy``
       | Builds a string composed of the concatenation of Kafka topic, partition, and offset.

   * - FullKeyStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.FullKeyStrategy``
       | Uses complete key structure of the ``SinkDocument``.
       | Defaults to a blank document if no key exists.

   * - ProvidedInKeyStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy``
       | Uses the ``_id`` field specified in the key structure of the ``SinkDocument`` if it exists.
       | Throws an exception if the field is missing.

   * - ProvidedInValueStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy``
       | Uses the ``_id`` field specified in the value structure of the ``SinkDocument`` if it exists.
       | Throws an exception if the field is missing.

   * - PartialKeyStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.PartialKeyStrategy``
       | Uses a blacklist or whitelist projection of the key structure of the ``SinkDocument``.
       | Defaults to a blank document if no key exists.

   * - PartialValueStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy``
       | Uses a blacklist or whitelist projection of the value structure of the ``SinkDocument``.
       | Defaults to a blank document if no value exists.

   * - UuidStrategy
     - | Full Path: ``com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy``
       | Generates a random UUID as a string.

You can assign the ``document.id.strategy`` property as follows:

.. code-block:: properties

   document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy

To define a custom strategy, create a class that implements the interface
`IdStrategy
<https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/id/strategy/IdStrategy.java>`_
and provide the fully-qualified path to the ``document.id.strategy``
setting.

.. admonition:: Selected strategy may have implications on delivery semantics
   :class: note

   BSON ObjectId or UUID strategies can only guarantee at-least-once
   delivery since new ids would be generated on retries or re-processing.
   Other strategies permit exactly-once delivery if the fields that form
   the document *_id* are guaranteed to be unique.

.. _config-blacklist-whitelist:

Blacklist / Whitelist Projector
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section provides example projection configurations to show how they
filter the following sample record:

.. code-block:: json

   {
     "name": "Anonymous",
     "age": 42,
     "active": true,
     "address": {
       "city": "Unknown",
       "country": "NoWhereLand"
     },
     "food": [
       "Austrian",
       "Italian"
     ],
     "data": [
       {
         "k": "foo",
         "v": 1
       }
     ],
     "lut": {
       "key1": 12.34,
       "key2": 23.45
     },
     "destination: {
       "city": "Springfield",
       "country": "AnotherLand"
     }
   }

.. note::

   The following example configurations contain ``[key|value]``
   placeholder values that represent either ``key`` or ``value`` in order
   to avoid repetition. Specify the one appropriate to your use case when
   creating your configuration.

.. _blacklist-example:

Blacklist Projection Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the following example sink configuration, we specify a blacklist
projection and the specific fields to omit from the record:

.. code-block:: properties

   post.processor.chain=com.mongodb.kafka.connect.sink.processor.Blacklist[Key|Value]Projector
   [key|value].projection.type=blacklist
   [key|value].projection.list=age,address.city,lut.key2,data.v

.. note::

   You can use the "." (dot) notation to reference subdocuments in the
   record. You can also use it to reference fields of documents within an
   array.

The record contains the following data after applying the projection:

.. code-block:: json

   {
     "name": "Anonymous",
     "active": true,
     "address": {
       "country": "NoWhereLand"
     },
     "food": [
       "Austrian",
       "Italian"
     ],
     "data": [
       {
         "k": "foo"
       }
     ],
     "lut": {
       "key1": 12.34
     },
     "destination: {
       "city": "Springfield",
       "country": "AnotherLand"
     }
   }

.. _whitelist-example:

Whitelist Projection Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the following example sink configuration, we specify a whitelist
projection and the specific fields to include in the record:

.. code-block:: properties

   post.processor.chain=com.mongodb.kafka.connect.sink.processor.Whitelist[Key|Value]Projector
   [key|value].projection.type=whitelist
   [key|value].projection.list=age,address.city,lut.key2,data.v

.. note::

   You can use the "." notation to reference subdocuments in the record. You
   can also use it to reference fields of documents within an array.

The record contains the following data after applying the projection:

.. code-block:: json

   {
     "age": 42,
     "address": {
       "city": "Unknown"
     },
     "data": [
       {
         "v": 1
       }
     ],
     "lut": {
       "key2": 23.45
     }
   }

Projection Wildcard Pattern Matching
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The previous example projection configurations demonstrated exact
string matching on field names. The projection ``list`` setting also supports
the following wildcard patterns matching on field names:

* "``*``" (`star`): matches a string of any length for the level in the
  document in which it is specified.

* "``**``" (`double star`): matches the current and all nested levels from
  which it is specified.

The examples below demonstrate how to use each wildcard pattern and the
projection output from the following sample record:

.. code-block:: json

   {
     "forecast": [
       { "day": "Monday",
         "temp": {
           "degrees": 25.3,
           "units": "C",
           "range": {
             "low": 23.0,
             "high": 28.7
           }
         },
         "uv": 5
       }
     ],
     "city": "Springfield",
     "population: {
       "qty": 30.7,
       "scale": 1000,
       "units": "people"
     }
   }

**Whitelist Wildcard Examples**

The ``*`` wildcard pattern in the example below matches all the keys named
``temp`` in the ``forecast`` array and all fields nested a single level
below it.

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=city,forecast.temp.*

The record contains the following data after applying the projection:

.. code-block:: json

   {
     "city": "Springfield",
     "forecast": [
       {
         "temp": {
           "degrees": 25.3,
           "units": "C",
           "range": {
             "low": 23.0,
             "high": 28.7
           }
         }
       }
     ]
   }

The ``**`` wildcard pattern in the example below matches all the keys for
all levels that contain the field ``scale``.

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=**.scale

The record contains the following data after applying the projection:

.. code-block:: json

   {
     "population: {
       "qty": 30.7,
       "scale": 1000,
       "units": "people"
     }
   }

**Blacklist Wildcard Examples**

The wildcard character can also be used to match all field names at 
specific levels as demonstrated in the following blacklist projection 
configuration example:

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=population,forecast.*.*

The record contains the following data after applying the projection:

.. code-block:: json

   {
     "forecast": [
       { "day": "Monday",
         "uv": 5
       }
     ],
     "city": "Springfield",
   }

   
The connector configuration also supports the ``**`` (double star)
wildcard which matches the current and all nested levels from which it is
specified.

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=**.high
   
   {
     "city": "Springfield",
     "population: {
       "qty": 30.7,
       "scale": 1000,
       "units": "people"
     }
   }

.. _config-field-renaming:

Field Renaming Post Processors
~~~~~~~~~~~~----------------~

This section provides example configurations for the ``RenameByMapping``
and ``RenameByRegex`` post processors to show how they update field names
in a sink record. The field renaming parameters specify whether to update the
``key`` or ``value`` document in the record using dot notation as well as
the pattern to match and replacement string in a JSON array.

The field renaming post processor examples use the following sample sink
record:

**Key document**

.. code-block:: json

   {
     "location": "Provence",
     "date_month": "October",
     "date_day": 17
   }

**Value document**

.. code-block:: json

   {
     "flapjacks": {
       "purchased": 598,
       "size": "large"
     }
   }

.. _field-renaming-mapping-example:

RenameByMapping Example
^^^^^^^^^^^^^^^^^^^^^^^

The ``RenameByMapping`` post processor setting is an array of objects.
Each object in the array contains the following JSON element keys:

Each object contains the text to match in the ``oldName`` element and
the replacement text in the ``newName`` element.

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1,2

   * - Key Name
     - Description

   * - oldName  
     - Contains a string that matches on the text to replace.
  
   * - newName
     - Contains the replacement text for all matches of the string defined
       in the ``oldName`` field.

.. code-block:: properties

   field.renamer.mapping=[{"oldName":"key.location","newName":"city"},{"oldName":"value.flapjacks","newName":"crepes"}]

The record contains the following data after applying the ``RenameByMapping``
post processor:

**Key document**

.. code-block:: json
   :emphasize-lines: 2

   {
     "city": "Provence",
     "date_month": "October",
     "date_day": 17
   }

**Value document**

.. code-block:: json
   :emphasize-lines: 2

   {
     "crepes": {
       "purchased": 598,
       "size": "large"
     }
   }

.. _field-renaming-regex-example:

RenameByRegex
^^^^^^^^^^^^^

The ``RenameByRegex`` post processor setting is an array of objects.
Each object in the array contains the following JSON element keys:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1,2

   * - Key Name
     - Description

   * - regexp
     - Contains a regular expression that matches fields to perform the 
       replacement.

   * - pattern
     - Contains a regular expression that matches on the text to replace.

   * - newName
     - Contains the replacement text for all matches of the regular expression
       defined in the ``pattern`` field.


.. code-block:: properties

   field.renamer.mapping=[{"regexp":"^key\\.date.*$","pattern":"_","replace":"-"},{"regexp":"^value\\.crepes\\..*","pattern":"purchased","replace":"quantity"}]

The record contains the following data after applying the ``RenameByMapping``
post processor:

**Key document**

.. code-block:: json
   :emphasize-lines: 3,4

   {
     "city": "Provence",
     "date-month": "October",
     "date-day": 17
   }

**Value document**

.. code-block:: json
   :emphasize-lines: 3

   {
     "crepes": {
       "quantity": 598,
       "size": "large"
     }
   }

The post processor applied the following changes:

* All field names in the key document of the sink record that started with
  "date" are matched. In the matched fields, all instances of "_" are
  replaced with "-".
* All field names in the value document of the sink record that are
  subdocuments of ``crepes`` are matched. In the matched fields, all
  instances of "purchased" are replaced with "quantity".

.. admonition:: Ensure renaming does not result in duplicate keys in the same document

   The renaming post processors update the key fields of a JSON document
   which can result in duplicate keys within a document. The renaming post
   processor skips the renaming step if the replacement key already exists
   at the current level.

.. _custom-write-models:

Custom Write Models
-------------------

TODO

The default behaviour for the connector whenever documents are written to MongoDB collections is to make use of a proper
`ReplaceOneModel <http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/ReplaceOneModel.html>`_ with
`upsert mode
<http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/ReplaceOneModel.html>`_
and **create the filter document based on the _id field** which results
from applying the configured ``DocumentIdAdder`` in the value structure of the sink document.

However, there are other use cases which need different approaches and the **customization option for generating custom write models**
can support these. The configuration entry (\ *mongodb.writemodel.strategy*\ ) allows for such customizations. Currently, the following
strategies are implemented:


* **default behaviour** com.mongodb.kafka.connect.sink.writemodel.strategy.\ **ReplaceOneDefaultStrategy**
* **business key** (see `use case 1 <https://github.com/mongodb/mongo-kafka#use-case-1-employing-business-keys>`_ below)
  com.mongodb.kafka.connect.sink.writemodel.strategy.\ **ReplaceOneBusinessKeyStrategy**
* **delete on null values** com.mongodb.kafka.connect.sink.writemodel.strategy.\ **DeleteOneDefaultStrategy** implicitly used when
  config option *mongodb.delete.on.null.values=true* for `convention-based deletion <https://github.com/mongodb/mongo-kafka#convention-based-deletion-on-null-values>`_
* **add inserted/modified timestamps** (see `use case 2 <https://github.com/mongodb/mongo-kafka#use-case-2-add-inserted-and-modified-timestamps>`_ below)
  com.mongodb.kafka.connect.sink.writemodel.strategy.\ **UpdateOneTimestampsStrategy**

.. note::

   Future versions will allow to make use of arbitrary, individual strategies
   that can be registered and easily used as *mongodb.writemodel.strategy*
   configuration setting.

Use Case 1: Employing Business Keys
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this use case, we want to store records that are unique by the business
key field
but also want to generate new BSON ObjectIds for each document.

identify records by a unique business key
already assigned
in the Kafka data, but

Let's say you want to re-use a unique business key found in your sink
records while at the same time have *BSON ObjectIds* created for
the resulting MongoDB documents.

To achieve this a few simple configuration steps are necessary:


#. Make sure to **create a unique key constraint** for the business key of your target MongoDB collection.
#. Use the ``PartialValueStrategy`` as the ``DocumentIdAdder``'s strategy in order to let the connector know which fields belong to the
   business key.
#. Use the ``ReplaceOneBusinessKeyStrategy`` instead of the default behaviour.

These configuration settings then allow to have **filter documents based
on the original business key but still have *BSON ObjectIds*
created for the _id field** during the first upsert into your target MongoDB target collection. Find below how such a setup might look like:

Given the following Kafka record:

.. code-block:: json

   {
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

Together with the sink connector config:

.. code-block:: json

   {
     "name": "mongo-sink",
     "config": {
       ...
       "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy",
       "value.projection.list": "fieldA,fieldB",
       "value.projection.type": "whitelist",
       "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy"
     }
   }

This will create a MongoDB document looking like:

.. code-block:: json

   {
     "_id": ObjectId('5abf52cc97e51aae0679d237'),
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

All upsert operations are done based on the unique business key which for this example is a compound one that consists of the two fields *(fieldA,fieldB)*.

Use Case 2: Add Inserted and Modified Timestamps
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's say you want to attach timestamps to the resulting MongoDB documents such that you can store the point in time of the document insertion and at the same time maintain a second timestamp reflecting when a document was modified.

All that needs to be done is use the **UpdateOneTimestampsStrategy** instead of the default behaviour. What results from this is that
the custom write model will take care of attaching two timestamps to MongoDB documents:

1) **_insertedTS**\ : will only be set once in case the upsert operation results in a new MongoDB document being inserted into the corresponding collection
2) **_modifiedTS**\ : will be set each time the upsert operation results in an existing MongoDB document being updated in the corresponding collection

Given the following Kafka record

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

Together with the sink connector config:

.. code-block:: json

   {
     "name": "mdb-sink",
     "config": {
       ...
       "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy",
       "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy"
     }
   }

This will create a MongoDB document looking like:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "_insertedTS": ISODate('2018-07-22T09:19:000Z"),
     "_modifiedTS": ISODate("2018-07-22T09:19:000Z"),
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

If at some point in time later there was a Kafka record referring to the same _id but containing updated data:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "fieldA": "anonymous",
     "fieldB": -23,
     "active": false,
     "values": [
       12.34,
       23.45
     ]
   }

Then the existing MongoDB document will get updated together with a fresh timestamp for the **_modifiedTS** value:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "_insertedTS": "ISODate('2018-07-22T09:19:000Z')",
     "_modifiedTS": "ISODate('2018-07-31T19:09:000Z')",
     "fieldA": "anonymous",
     "fieldB": -23,
     "active": false,
     "values": [
       12.34,
       23.45
     ]
   }
