.kafka-sink-postprocessors

====================================
Kafka Sink Connector Post-Processors
====================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecols

Post Processing of Documents
----------------------------

Post processors are sink connector classes that modify the
``SinkDocument``, a class that contains BSON representation of the
``SinkRecord`` key and value fields, after the data has been read and
deserialized from the Kafka topic.


data before it is saved to MongoDB.


A ``SinkDocument``
contains BSON representations of the ``SinkRecord`` key and value
fields``.


The connector applies a **chain of post processors** to the
``SinkDocument`` and stores the resulting data to a MongoDB collection.

The 'post.processor.chain' configuration property allows users to customize the post processor chain applied to the converted records
before they are written to the sink. Just specify a comma separated list of fully qualified class names which provide the post processor
implementations, either existing ones or new/customized ones, like so:

.. code-block:: properties

   post.processor.chain=com.mongodb.kafka.connect.sink.processor.field.renaming.RenameByMapping

The ``DocumentIdAdder`` is automatically added at the very first position in the chain in case it is not present. Other than that, the chain
can be built more or less arbitrarily.

The post processors included

.. list-table:
   :header-rows: 1
   :stub-columns: 1

   * - Name
     - Description

   * - DocumentIdAdder
     - | uses the configured *strategy* (explained below) to insert an **_id field**
       | Required



``SinkDocument``  undergo a **chain of post processors**. There are
four processors available
Choose from the four processors
The four processors are as follows


Once the the data has been deserialized by the converters, the
``SinkRecord`` is converted into a ``SinkDocument``.


the SinkRecord

After deserialization

the conversion into a BSON document, the documents undergo a **chain of post processors**. There are 4 processors to choose from:

* **DocumentIdAdder** (mandatory): uses the configured *strategy* (explained below) to insert an **_id field**
* **BlacklistProjector** (optional): applicable for *key* + *value* structure
* **WhitelistProjector** (optional): applicable for *key* + *value* structure
* **FieldRenamer** (optional): applicable for *key* + *value* structure

You can create a custom post processor by extending the provided abstract
base class
`PostProcessor
<https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/PostProcessor.java>`_.
A few use cases of a custom post processor include:

* Remove fields with null values
* Redact any fields containing sensitive information
* etc.


The next section covers configuring the default post processors:

DocumentIdAdder
~~~~~~~~~~~~~~~

The sink connector is able to process both, the key and value parts of
kafka records. After the conversion to MongoDB BSON documents,
an *_id* field is automatically added to value documents which are finally persisted in a MongoDB collection. The *_id* itself is filled by
the **configured document id generation strategy**\ , which can be one of the following:


* A MongoDB **BSON ObjectId** (default)
* A Java **UUID**
* **Kafka meta-data** comprised of the string concatenation based on [topic-partition-offset] information
* **full key** using the sink record's complete key structure
* **provided in key** expects the sink record's key to contain an *_id* field which is used as is (error if not present or null)
* **provided in value** expects the sink record's value to contain an *_id* field which is used as is (error if not present or null)
* **partial key** using parts of the sink record's key structure
* **partial value** using parts of the sink record's value structure

.. note::

   The latter two of which can be configured to use the blacklist/whitelist
   field projection mechanisms described below.

The strategy is set by means of the following property:

.. code-block:: properties

   document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy

There is a configuration property which allows to customize the applied id generation strategy. Thus, if none of the available strategies
fits your needs, further strategies can be easily implemented based on the interface
`IdStrategy <https://github.com/mongodb/mongo-kafka/blob/master/src/main/java/com/mongodb/kafka/connect/sink/processor/id/strategy/IdStrategy.java>`_

All custom strategies that should be available to the connector can be registered by specifying a list of fully qualified class names
for the following configuration property:

.. code-block:: properties

   document.id.strategies=...

**It's important to keep in mind that the chosen / implemented id strategy has direct implications on the possible delivery semantics.**
If it's set to BSON ObjectId or UUID respectively, it can only ever guarantee at-least-once delivery of records, since new ids will result
due to the re-processing on retries after failures. The other strategies permit exactly-once semantics if the respective fields forming
the document *_id* are guaranteed to be unique in the first place.

Blacklist / Whitelist Projector (optional)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default the current implementation converts and persists the full value structure of the sink records.
Key and/or value handling can be configured by using either a **blacklist or whitelist** approach in order to remove/keep fields
from the record's structure. By using the "." notation to access sub documents it's also supported to do
redaction of nested fields. It is also possible to refer to fields of documents found within arrays by the same notation.
See two concrete examples below about the behaviour of these two projection strategies.

Given the following fictional data record:

.. code-block:: json

   {
     "name": "Anonymous",
     "age": 42,
     "active": true,
     "address": {
       "city": "Unknown",
       "country": "NoWhereLand"
     },
     "food": [
       "Austrian",
       "Italian"
     ],
     "data": [
       {
         "k": "foo",
         "v": 1
       }
     ],
     "lut": {
       "key1": 12.34,
       "key2": 23.45
     }
   }

Example blacklist projection:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=age,address.city,lut.key2,data.v

will result in:

.. code-block:: json

   {
     "name": "Anonymous",
     "active": true,
     "address": {
       "country": "NoWhereLand"
     },
     "food": [
       "Austrian",
       "Italian"
     ],
     "data": [
       {
         "k": "foo"
       }
     ],
     "lut": {
       "key1": 12.34
     }
   }

Example whitelist projection:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=age,address.city,lut.key2,data.v

will result in:

.. code-block:: json

   {
     "age": 42,
     "address": {
       "city": "Unknown"
     },
     "data": [
       {
         "v": 1
       }
     ],
     "lut": {
       "key2": 23.45
     }
   }

To have more flexibility in this regard there might be future support for:


* Explicit null handling: the option to preserve / ignore fields with null values
* Investigate if it makes sense to support array element access for field projections based on an index or a given value to project
  simple/primitive type elements

How wildcard pattern matching works:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The configuration supports wildcard matching using a **'*'** character notation. A wildcard
is supported on any level in the document structure in order to include (whitelist) or
exclude (blacklist) any fieldname at the corresponding level. A part from that there is support
for **'**'** which can be used at any level to include/exclude the full sub structure
(i.e. all nested levels further down in the hierarchy).


.. note::

   A bunch of more concrete examples of field projections including wildcard
   pattern matching can be found in a corresponding `test class
   <https://github.com/mongodb/mongo-kafka/blob/master/src/test/java/com/mongodb/kafka/connect/sink/processor/field/projection/FieldProjectorTest.java>`_.

Whitelist examples:
~~~~~~~~~~~~~~~~~~~

The following example will include the *age* field, the *lut* field and all its immediate sub-fields (i.e. one level down):

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=age,lut.*

The following example will include the *active* field, the *address* field and its full sub structure (all available nested levels):

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=active,address.**

The final whitelist example will include all fields on the 1st and 2nd level:

.. code-block:: properties

   [key|value].projection.type=whitelist
   [key|value].projection.list=*.*

Blacklist examples:
~~~~~~~~~~~~~~~~~~~

The following example will exclude the *age* field, the *lut* field and all its immediate subfields (i.e. one level down):

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=age,lut.*

The following example will exclude the *active* field, the *address* field and its full sub structure (all available nested levels):

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=active,address.**

The final blacklist example will exclude: all fields on the 1st and 2nd level:

.. code-block:: properties

   [key|value].projection.type=blacklist
   [key|value].projection.list=*.*

Field Renaming (optional)
~~~~~~~~~~~~~~~~~~~~~~~~~

There are two different options to rename any fields in the record, namely a simple and rigid 1:1 field name mapping or a more
flexible approach using regular expressions. Both config options are defined by inline JSON arrays containing objects which describe the renaming.

Example 1:

.. code-block:: properties

   field.renamer.mapping=[{"oldName":"key.fieldA","newName":"field1"},{"oldName":"value.xyz","newName":"abc"}]

Will:


#. Rename field ``fieldA`` to ``field1`` in the **key document structure**
#. Rename field ``xyz`` to ``abc`` in the **value document structure**

Example 2:

.. code-block:: properties

   field.renamer.mapping=[{"regexp":"^key\\..*my.*$","pattern":"my","replace":""},{"regexp":"^value\\..*-.+$","pattern":"-","replace":"_"}]

These settings cause:


#. **All field names of the key structure containing 'my'** to be renamed so that **'my' is removed**
#. **All field names of the value structure containing a '-'** to be renamed by replacing **'-' with '_'**

.. note::

   The use of the **"." character** as navigational operator in both examples.
   It's used in order to refer to nested fields in subdocuments of the record
   structure. The prefix at the very beginning is used as a simple convention
   to distinguish between the *key* and *value* structure of a document.

Custom Write Models
^^^^^^^^^^^^^^^^^^^

The default behaviour for the connector whenever documents are written to MongoDB collections is to make use of a proper
`ReplaceOneModel <http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/ReplaceOneModel.html>`_ with
`upsert mode <http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/ReplaceOneModel.html>`_ and **create the filter document based on the _id field** which results from applying the configured DocumentIdAdder in the value structure of the sink document.

However, there are other use cases which need different approaches and the **customization option for generating custom write models**
can support these. The configuration entry (\ *mongodb.writemodel.strategy*\ ) allows for such customizations. Currently, the following
strategies are implemented:


* **default behaviour** com.mongodb.kafka.connect.sink.writemodel.strategy.\ **ReplaceOneDefaultStrategy**
* **business key** (see `use case 1 <https://github.com/mongodb/mongo-kafka#use-case-1-employing-business-keys>`_ below)
  com.mongodb.kafka.connect.sink.writemodel.strategy.\ **ReplaceOneBusinessKeyStrategy**
* **delete on null values** com.mongodb.kafka.connect.sink.writemodel.strategy.\ **DeleteOneDefaultStrategy** implicitly used when
  config option *mongodb.delete.on.null.values=true* for `convention-based deletion <https://github.com/mongodb/mongo-kafka#convention-based-deletion-on-null-values>`_
* **add inserted/modified timestamps** (see `use case 2 <https://github.com/mongodb/mongo-kafka#use-case-2-add-inserted-and-modified-timestamps>`_ below)
  com.mongodb.kafka.connect.sink.writemodel.strategy.\ **UpdateOneTimestampsStrategy**

.. note::

   Future versions will allow to make use of arbitrary, individual strategies
   that can be registered and easily used as *mongodb.writemodel.strategy*
   configuration setting.

Use Case 1: Employing Business Keys
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's say you want to re-use a unique business key found in your sink
records while at the same time have *BSON ObjectIds* created for
the resulting MongoDB documents.

To achieve this a few simple configuration steps are necessary:


#. Make sure to **create a unique key constraint** for the business key of your target MongoDB collection.
#. Use the **PartialValueStrategy** as the DocumentIdAdder's strategy in order to let the connector know which fields belong to the
   business key.
#. Use the **ReplaceOneBusinessKeyStrategy** instead of the default behaviour.

These configuration settings then allow to have **filter documents based
on the original business key but still have *BSON ObjectIds*
created for the _id field** during the first upsert into your target MongoDB target collection. Find below how such a setup might look like:

Given the following Kafka record:

.. code-block:: json

   {
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

Together with the sink connector config:

.. code-block:: json

   {
     "name": "mongo-sink",
     "config": {
       ...
       "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy",
       "value.projection.list": "fieldA,fieldB",
       "value.projection.type": "whitelist",
       "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy"
     }
   }

This will create a MongoDB document looking like:

.. code-block:: json

   {
     "_id": ObjectId('5abf52cc97e51aae0679d237'),
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

All upsert operations are done based on the unique business key which for this example is a compound one that consists of the two fields *(fieldA,fieldB)*.

Use Case 2: Add Inserted and Modified Timestamps
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's say you want to attach timestamps to the resulting MongoDB documents such that you can store the point in time of the document insertion and at the same time maintain a second timestamp reflecting when a document was modified.

All that needs to be done is use the **UpdateOneTimestampsStrategy** instead of the default behaviour. What results from this is that
the custom write model will take care of attaching two timestamps to MongoDB documents:

1) **_insertedTS**\ : will only be set once in case the upsert operation results in a new MongoDB document being inserted into the corresponding collection
2) **_modifiedTS**\ : will be set each time the upsert operation results in an existing MongoDB document being updated in the corresponding collection

Given the following Kafka record

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

Together with the sink connector config:

.. code-block:: json

   {
     "name": "mdb-sink",
     "config": {
       ...
       "document.id.strategy": "com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy",
       "writemodel.strategy": "com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy"
     }
   }

This will create a MongoDB document looking like:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "_insertedTS": ISODate('2018-07-22T09:19:000Z"),
     "_modifiedTS": ISODate("2018-07-22T09:19:000Z"),
     "fieldA": "Anonymous",
     "fieldB": 42,
     "active": true,
     "values": [
       12.34,
       23.45,
       34.56,
       45.67
     ]
   }

If at some point in time later there was a Kafka record referring to the same _id but containing updated data:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "fieldA": "anonymous",
     "fieldB": -23,
     "active": false,
     "values": [
       12.34,
       23.45
     ]
   }

Then the existing MongoDB document will get updated together with a fresh timestamp for the **_modifiedTS** value:

.. code-block:: json

   {
     "_id": "ABCD-1234",
     "_insertedTS": "ISODate('2018-07-22T09:19:000Z')",
     "_modifiedTS": "ISODate('2018-07-31T19:09:000Z')",
     "fieldA": "anonymous",
     "fieldB": -23,
     "active": false,
     "values": [
       12.34,
       23.45
     ]
   }
